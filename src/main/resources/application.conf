
ollama {
  host = "http://localhost:11434"
  model = "llama3.2:latest"
  request-timeout-seconds = 500
  max-turns = 10  # Maximum conversation turns
  min-response-length = 10  # Minimum response length to continue conversation
}

grpc {
  server {
    host = "localhost"
    port = 50051
  }
}

http {
  interface = "0.0.0.0"
  port = 8080
}